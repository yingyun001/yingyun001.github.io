<div class="container">
<div class="panel-group" id="accordion">
   <div class="panel panel-default">
      <div class="panel-heading">
         <h3 class="panel-title">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseOne">
               Quality of Service（服务质量）
            </a>
         </h3>
      </div>
      <div id="collapseOne" class="panel-collapse collapse">
         <div class="panel-body">
            <div class="well well-sm">
               ...<br/>
                 &nbsp;&nbsp;&lt;forward mode='nat' dev='eth0'/&gt;<br/>
                 &nbsp;&nbsp;&lt;bandwidth&gt;<br/>
                   &nbsp;&nbsp;&nbsp;&nbsp;&lt;inbound average='1000' peak='5000' burst='5120'/&gt;<br/>
                   &nbsp;&nbsp;&nbsp;&nbsp;&lt;outbound average='128' peak='256' burst='256'/&gt;<br/>
                   &nbsp;&nbsp;&lt;/bandwidth&gt;<br/>
               ...
            </div>
            <p class="alert alert-info">
            &lt;bandwidth&gt; 标签设置了特定网络的服务质量。只有在拥有 &lt;forward&gt; 标签，且标签中有值为 route，nat 的 mode 属性或者压根没有 mode 属性的网络（例如独立网络）才能够设置 bandwidth。但如果 &lt;forward&gt; 标签中 mode 属性的值是 bridge，passthrough，private 或者 hostdev 的话是不能够设置 bandwidth 的。如果您执意这么做，那么您是不能够定义网络或者创建瞬态网络的。<br/><br/>
            &lt;bandwidth&gt; 元素只能作为域中的 &lt;interface&gt; 标签中的一个子标签，&lt;network&gt; 标签中的子标签, 或者是 &lt;network&gt; 标签下的 &lt;portgroup&gt; 标签中的子标签。<br/><br/>
            作为域的 &lt;interface&gt; 的一个子标签，该带宽只适用于域的一个接口。作为 &lt;network&gt; 标签的一个子标签，这个 bandwidth 指的是经过绑定了该 network 的所有虚拟机上的网卡带宽的聚合，而不是指单独虚拟机的虚拟网卡的带宽。如果域的 &lt;interface&gt; 有 &lt;bandwidth&gt; 子标签，且值比整个网络的总和都大，那么其实 &lt;network&gt; 中的值才会生效。这是因为这两个瓶颈是相互独立的，域的 &lt;interface&gt; 标签下的宽带（bandwidth）适用于接口的开发设备，而 &lt;network&gt; 宽带控制适用于为该网络创建的网桥设备中的接口部分。<br/><br/>
            作为 &lt;network&gt; 标签下的 &lt;portgroup&gt; 标签中的子标签，如果域的 &lt;interface&gt; 标签下的 &lt;source&gt; 标签中有 portgroup 属性且 &lt;interface&gt; 下没有 &lt;bandwidth&gt; 标签，而后 portgroup 的 &lt;bandwidth&gt; 标签会应用于定义为 portgroup 成员的每个独立的虚拟机中。域的 &lt;interface&gt; 标签下任意的 &lt;bandwidth&gt; 标签将会修写 portgroup 中的设置。<br/><br/> 
            Incoming 和 outgoing 流量可以被单独形成。bandwidth 子标签最多只能有一个 inbound 和一个 outbound 子元素。脱离这个inbound 和 outbound 的任何一个都可能导致 QoS 无法正常应用到系统当中。那么，如果您仅想对 incoming 流量进行设置的话，使用 inbound 就可以了，反之亦然。每个元素都有一个元数据属性 - average（或像下面描述的 floor）。下面将会介绍所有属性，每个属性必须是一个整型数字才是合法的。<br/><br/>
            </p>
            <p class="bg-danger">
               <i>平均值</i><br/>
               &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
               指定了虚拟机虚拟网卡的平均比特率。（单位：KB/s）<br/><br/>
               <i>峰值</i><br/>
               &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
               这是一个可选的属性，它指定了网桥发送数据的最大速率（单位：KB/s）。需要注意的是限制条件：outbound 元素中的这个属性是被忽略的（因为 Linux 的入口过滤器还不知道它。）<br/><br/>
               <i>burst</i><br/>
               &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
               这也是一个可选的属性，它指定了以<i>峰值</i>这个速度一次 burst 中的传输的最大值（单位：KB）。<br/><br/>
               <i>floor</i><br/>
               &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
               同样，floor 也是一个可选的属性，它仅可以在 inbound 元素中使用。该属性用于保证虚拟机虚拟网卡的最小吞吐量。但是，它需要所有的流量都经过 QoS 策略起作用的那个点，这也是为什么该属性目前只可作用在虚拟网络上的原因。但是，绑定了该虚拟网卡的虚拟网络需要至少一个 inbound QoS 设置（设置 inbound 的时候，average 是必不可少的。）但是，如果您使用了 floor 这个属性，您就不必再去设置平均值（average）了，而峰值（peak）和 burst 属性仍然需要平均值（average）。目前，Linux 内核还不允许入口队列规则（ingress qdiscs）有任何子类，所以这也就是为什么 inbound 元素下可以有 floor 属性而 outbound 元素下不能有该属性的原因。
            </p> 
            <p>
               NAME
       tbf - Token Bucket Filter

SYNOPSIS
       tc  qdisc  ...  tbf rate rate burst bytes/cell ( latency ms | limit bytes ) [ mpu bytes [ peakrate
       rate mtu bytes/cell ] ]

       burst is also known as buffer and maxburst. mtu is also known as minburst.

DESCRIPTION
       The Token Bucket Filter is a classless queueing discipline available for traffic control with  the
       tc(8) command.

       TBF  is  a  pure  shaper  and  never schedules traffic. It is non-work-conserving and may throttle
       itself, although packets are available, to ensure that the configured rate is not exceeded.  It is
       able  to  shape  up  to  1mbit/s of normal traffic with ideal minimal burstiness, sending out data
       exactly at the configured rates.

       Much higher rates are possible but at the cost of losing the minimal  burstiness.  In  that  case,
       data  is  on  average  dequeued  at the configured rate but may be sent much faster at millisecond
       timescales. Because of further queues living in network adaptors, this is often not a problem.

ALGORITHM
       As the name implies, traffic is filtered based on the expenditure of tokens.  Tokens roughly  cor‐
       respond  to bytes, with the additional constraint that each packet consumes some tokens, no matter
       how small it is. This reflects the fact that even a zero-sized packet occupies the link  for  some
       time.

       On  creation, the TBF is stocked with tokens which correspond to the amount of traffic that can be
       burst in one go. Tokens arrive at a steady rate, until the bucket is full.

       If no tokens are available, packets are queued, up to a configured limit. The TBF  now  calculates
       the token deficit, and throttles until the first packet in the queue can be sent.

       If  it  is  not  acceptable to burst out packets at maximum speed, a peakrate can be configured to
       limit the speed at which the bucket empties. This peakrate is implemented as a second TBF  with  a
       very small bucket, so that it doesn't burst.

       To achieve perfection, the second bucket may contain only a single packet, which leads to the ear‐
       lier mentioned 1mbit/s limit.

       This limit is caused by the fact that the kernel can only throttle for at minimum 1 'jiffy', which
       depends  on  HZ  as  1/HZ.  For perfect shaping, only a single packet can get sent per jiffy - for
       HZ=100, this means 100 packets of on  average  1000  bytes  each,  which  roughly  corresponds  to
       1mbit/s.

PARAMETERS
       See tc(8) for how to specify the units of these values.

       limit or latency
              Limit is the number of bytes that can be queued waiting for tokens to become available. You
              can also specify this the other way around by setting the latency parameter,  which  speci‐
              fies  the  maximum amount of time a packet can sit in the TBF. The latter calculation takes
              into account the size of the bucket, the rate and possibly the peakrate (if set). These two
              parameters are mutually exclusive.

       burst  Also known as buffer or maxburst.  Size of the bucket, in bytes. This is the maximum amount
              of bytes that tokens can be available for  instantaneously.   In  general,  larger  shaping
              rates  require  a larger buffer. For 10mbit/s on Intel, you need at least 10kbyte buffer if
              you want to reach your configured rate!

              If your buffer is too small, packets may be dropped because more tokens  arrive  per  timer
              tick  than  fit  in your bucket.  The minimum buffer size can be calculated by dividing the
              rate by HZ.

              Token usage calculations are performed using a table which by default has a resolution of 8
              packets.   This  resolution  can be changed by specifying the cell size with the burst. For
              example, to specify a 6000 byte buffer with a 16 byte cell size, set a  burst  of  6000/16.
              You will probably never have to set this. Must be an integral power of 2.

       mpu    A  zero-sized packet does not use zero bandwidth. For ethernet, no packet uses less than 64
              bytes. The Minimum Packet Unit determines the minimal token usage (specified in bytes)  for
              a packet. Defaults to zero.

       rate   The speed knob. See remarks above about limits! See tc(8) for units.

       Furthermore, if a peakrate is desired, the following parameters are available:

       peakrate
              Maximum  depletion  rate  of  the bucket.  The peakrate does not need to be set, it is only
              necessary if perfect millisecond timescale shaping is required.

       mtu/minburst
              Specifies the size of the peakrate bucket. For perfect accuracy, should be set to  the  MTU
              of  the  interface.   If a peakrate is needed, but some burstiness is acceptable, this size
              can be raised. A 3000 byte minburst allows around 3mbit/s  of  peakrate,  given  1000  byte
              packets.

              Like the regular burstsize you can also specify a cell size.

EXAMPLE & USAGE
       To  attach  a TBF with a sustained maximum rate of 0.5mbit/s, a peakrate of 1.0mbit/s, a 5kilobyte
       buffer, with a pre-bucket queue size limit calculated so the TBF causes at most 70ms  of  latency,
       with perfect peakrate behaviour, issue:

       # tc qdisc add dev eth0 root tbf rate 0.5mbit \
         burst 5kb latency 70ms peakrate 1mbit       \
         minburst 1540

SEE ALSO
       tc(8)

AUTHOR
       Alexey N. Kuznetsov, <kuznet@ms2.inr.ac.ru>. This manpage maintained by bert hubert <ahu@ds9a.nl>

iproute2                                     13 December 2001                                       TC(8)

            </p>
         </div>
      </div>
   </div>
</div>
